---
title: "Testing"
description: |
  A short description of the post.
author:
  - name: Joshua Kunst
    url: http://jkunst.com/
date: 07-24-2020
output:
  distill::distill_article:
    self_contained: false
draft: true
---


Suppose we have a (small) dataset, and a particular statistic $T$ (ie quantity computable from data) we are interested in.
The Bias/Variance tradeoff tells us that if our statistic $T$ is an unbiased estimator of some quantity $\theta$, then the mean square error $\mathbb{E}[(\theta-T)^2]$ is completely determined by the variance of our statistic.
So understanding the variance is crucial to understanding how well our statistic is performing.

For simple enough statistics, we can determine the variance directly -- but for even slightly more complicated cases, the underlying mathematics quickly gets too difficult to handle gracefully.

This is where the *Bootstrap* enters the stage.
The Bootstrap is a way to estimate the distribution *of the statistic $T$* from the dataset itself, and then use that distribution to estimate new information about $T$ -- such as its variance, or forming confidence intervals for $T$ on a specific dataset, etc.

Basically, the Bootstrap approximates the true distribution on the data with the *empirical distribution* given by the data: in the empirical distribution, each data point has an equal probability $1/N$ of appearing.
Sampling from this empirical distribution amounts to sampling (with replacement!!) from the data itself.

## Example

For a concrete example, Wasserman (in All of Statistics, Example 8.6) picks up a dataset used by Bradley Efron to illustrate the Bootstrap:

```{r}
library(knitr)
library(tidyverse)
lsat.gpa = tibble(
  lsat = c(
    576, 635, 558, 578, 666, 580, 555, 661,
    651, 605, 653, 575, 545, 572, 594
  ),
  gpa  = c(
    3.39, 3.30, 2.81, 3.03, 3.44, 3.07, 3.00, 3.43,
    3.36, 3.13, 3.12, 2.74, 2.76, 2.88, 3.96
  )
)
ggplot(lsat.gpa, aes(lsat, gpa)) +
  geom_point()
```

The task is to estimate the correlation between LSAT scores and GPA scores from this set of `r NROW(lsat.gpa)` observations.
Whereas Wasserman looked at sample correlation, we can go one step further -- let's find distributions for a linear model: predicting GPA using LSAT.
Our model would be
\[
GPA \sim \beta_1 LSAT + \beta_0
\]
and easily calculated using `lm(gpa~lsat, lsat.gpa)`.
Computing this generates the model parameters as:

```{r}
lsat.gpa.lm = lm(gpa~lsat, lsat.gpa)
lsat.gpa.lm$coefficients %>% kable()
```

And we can plot the resulting fitted model with the data.

```{r}
ggplot(lsat.gpa, aes(lsat, gpa)) +
  geom_point() +
  geom_abline(slope = lsat.gpa.lm$coefficients["lsat"],
              intercept = lsat.gpa.lm$coefficients["(Intercept)"])
```

But this is just the expected values for intercept and slope for the model.
We do not yet know how stable these values are -- how large their respective variances come out to be.
This is where the Bootstrap steps in to help.

## The Bootstrap Method

The Bootstrap proceeds as follows:

1. Pick bootstrap size $B$.
2. $B$ times do:
   - Sample $N$ values $X^*_b$ with replacement from your data $X$
   - Calculate $T^*_b = T(X^*_b)$
3. Collect all the $T*^_b$: their distribution approximates the sampling distribution of $T$ on the original data.

So let's get ourselves bootstrap estimates of the slope and intercept.

```{r}
B = 1000
lsat.gpa.lm.boot = sapply(1:B, function(b) {
  lsat.gpa.star = lsat.gpa %>% sample_frac(replace = TRUE)
  lm(gpa~lsat, lsat.gpa.star)$coefficients
}) %>% t %>% as_tibble %>% rename(intercept=`(Intercept)`)
```

We can see the distribution of slope/intercept pairs in a heatmap as follows:

```{r}
ggplot(lsat.gpa.lm.boot,aes(intercept, lsat)) +
  geom_hex()
```